# Generative-AI-Model-Implementations

Implementation of Perceptrons, CNNs, and RNNs for AI experiments.

## ğŸ“Œ Project Overview

This repository contains implementations of fundamental AI and deep learning models, including:

- Rosenblattâ€™s Perceptron
- Manual 2D Convolution
- Convolutional Neural Networks (CNNs)
- Recurrent Neural Networks (RNNs)
- Hyperparameter tuning for CNNs and RNNs

Each implementation explores core AI/ML concepts and evaluates performance metrics.

## ğŸ› ï¸ Installation

### 1. Clone the Repository

```sh
git clone https://github.com/AzeemWaqarr/Generative-AI-Model-Implementations.git
cd Generative-AI-Model-Implementations
```

### 2. Install Dependencies

Make sure you have Python and Jupyter Notebook installed.

```sh
pip install -r requirements.txt  # (if applicable)
```

## ğŸ“‚ Project Structure

```
ğŸ“¦ GenerativeAI-Experiments
â”œâ”€â”€ ğŸ“„ README.md  # Project Documentation
â”œâ”€â”€ ğŸ“„ Report.pdf  # Assignment Report
â”œâ”€â”€ ğŸ“‚ notebooks  # Jupyter Notebooks for each question
â”‚   â”œâ”€â”€ Question_1.ipynb  # Perceptron Implementation
â”‚   â”œâ”€â”€ Question_2.ipynb  # Convolution from Scratch
â”‚   â”œâ”€â”€ Question_3.ipynb  # CNN for CIFAR-10
â”‚   â”œâ”€â”€ Question_4.ipynb  # RNN for Next-Word Prediction
â”‚   â”œâ”€â”€ Question_5.ipynb  # Hyperparameter Search
â””â”€â”€ ğŸ“‚ data  # (If any dataset is required)
```

## ğŸ“ Implementations

### **1ï¸âƒ£ Perceptron Implementation**

- Created a synthetic dataset and visualized it.
- Implemented a perceptron with a forward and backward pass.
- Evaluated decision boundary and accuracy.

### **2ï¸âƒ£ Convolution from Scratch**

- Applied edge detection, blurring, and sharpening filters.
- Compared convolution vs. correlation with different strides and padding.

### **3ï¸âƒ£ CNN for CIFAR-10**

- Built a CNN with convolution, pooling, and fully connected layers.
- Trained using categorical cross-entropy and Adam optimizer.
- Evaluated performance with accuracy, loss, and confusion matrix.

### **4ï¸âƒ£ Vanilla RNN for Next-Word Prediction**

- Tokenized Shakespeare text and trained a SimpleRNN model.
- Predicted the next 10 words given an input phrase.
- Evaluated performance with perplexity and accuracy.

### **5ï¸âƒ£ Hyperparameter Search**

- Tuned learning rate, number of layers, neurons, dropout, and optimizers.
- Used random search to automate testing.
- Identified the best-performing model configuration.

## ğŸ“Š Results

- Accuracy, loss curves, and decision boundaries are included in notebooks.
- Screenshots and visualizations are provided where applicable.

## ğŸ“¬ Contact

For any questions, feel free to reach out!

- **Email:** azeem.waqarr@example.com
- **GitHub:** [AzeemWaqarr](https://github.com/AzeemWaqarr)
