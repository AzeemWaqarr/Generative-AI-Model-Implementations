# Generative-AI-Model-Implementations

Implementation of Perceptrons, CNNs, and RNNs for AI experiments.

## 📌 Project Overview

This repository contains implementations of fundamental AI and deep learning models, including:

- Rosenblatt’s Perceptron
- Manual 2D Convolution
- Convolutional Neural Networks (CNNs)
- Recurrent Neural Networks (RNNs)
- Hyperparameter tuning for CNNs and RNNs

Each implementation explores core AI/ML concepts and evaluates performance metrics.

## 🛠️ Installation

### 1. Clone the Repository

```sh
git clone https://github.com/AzeemWaqarr/Generative-AI-Model-Implementations.git
cd Generative-AI-Model-Implementations
```

### 2. Install Dependencies

Make sure you have Python and Jupyter Notebook installed.

```sh
pip install -r requirements.txt  # (if applicable)
```

## 📂 Project Structure

```
📦 GenerativeAI-Experiments
├── 📄 README.md  # Project Documentation
├── 📄 Report.pdf  # Assignment Report
├── 📂 notebooks  # Jupyter Notebooks for each question
│   ├── Question_1.ipynb  # Perceptron Implementation
│   ├── Question_2.ipynb  # Convolution from Scratch
│   ├── Question_3.ipynb  # CNN for CIFAR-10
│   ├── Question_4.ipynb  # RNN for Next-Word Prediction
│   ├── Question_5.ipynb  # Hyperparameter Search
└── 📂 data  # (If any dataset is required)
```

## 📝 Implementations

### **1️⃣ Perceptron Implementation**

- Created a synthetic dataset and visualized it.
- Implemented a perceptron with a forward and backward pass.
- Evaluated decision boundary and accuracy.

### **2️⃣ Convolution from Scratch**

- Applied edge detection, blurring, and sharpening filters.
- Compared convolution vs. correlation with different strides and padding.

### **3️⃣ CNN for CIFAR-10**

- Built a CNN with convolution, pooling, and fully connected layers.
- Trained using categorical cross-entropy and Adam optimizer.
- Evaluated performance with accuracy, loss, and confusion matrix.

### **4️⃣ Vanilla RNN for Next-Word Prediction**

- Tokenized Shakespeare text and trained a SimpleRNN model.
- Predicted the next 10 words given an input phrase.
- Evaluated performance with perplexity and accuracy.

### **5️⃣ Hyperparameter Search**

- Tuned learning rate, number of layers, neurons, dropout, and optimizers.
- Used random search to automate testing.
- Identified the best-performing model configuration.

## 📊 Results

- Accuracy, loss curves, and decision boundaries are included in notebooks.
- Screenshots and visualizations are provided where applicable.

## 📬 Contact

For any questions, feel free to reach out!

- **Email:** azeem.waqarr@example.com
- **GitHub:** [AzeemWaqarr](https://github.com/AzeemWaqarr)
